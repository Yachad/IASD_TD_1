{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "'Python Interactive'",
      "language": "python",
      "name": "c745e46f-b9ab-4a1b-b77b-45f189b748d6"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "TD_LDA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hdXZdZF_ght",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Import libs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os \n",
        "import random\n",
        "from tqdm import tqdm\n",
        "random.seed(1234)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZQYs3_gHZt9",
        "colab_type": "text"
      },
      "source": [
        "![Texte alternatif…](https://github.com/Yachad/IASD_TD_1/blob/master/logo.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InloVtsJ_ghw",
        "colab_type": "text"
      },
      "source": [
        "## LDA from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_soXJnY_ghx",
        "colab_type": "text"
      },
      "source": [
        "###### Text clustering is a widely used techniques to automatically draw out patterns from a set of documents, specially for documents organizing or indexing (tagging).\n",
        "###### With the plethora amount of information available on the Internet, the topic of knowledge management has become ever so more important.\n",
        "###### Everyone’s way of thinking about things may differ ever so slightly, a team of information architects may argue for years over which word is the right term to represent a document. Tagging, on the other hand, users can use whatever term works for them.\n",
        "###### This is now a common way (e.g. on Twitter, StackOverflow) to sort relevant topics together so that they can be easily found by people of the same interested.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNBTZAR1_ghy",
        "colab_type": "text"
      },
      "source": [
        "## Latent Dirichlet Allocation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0aProyR_ghz",
        "colab_type": "text"
      },
      "source": [
        "**Latent Dirichlet Allocation** (LDA) is a probabilistic topic modeling method that gives us an approach to find out possible topics from documents that we do not know of beforehand. \n",
        "\n",
        "The key assumptions behind LDA is that each given documents is a mix of multiple topics. Given a set of documents, one can use the LDA framework to learn not only the topic mixture (distribution) that represents each document. But also word (distribution) that are associated with each topic to help understand what the topic might be referring to. \n",
        "\n",
        "The topic distribution for each document is distributed as \n",
        "\n",
        "$$ \\theta \\sim Dirichlet(\\alpha) $$\n",
        "\n",
        "Where $Dirichlet(\\alpha)$ denotes the Dirichlet distribution for parameter $\\alpha$.\n",
        "\n",
        "The term (word) distribution on the other hand is also modeled by a Dirichlet distribution, just under a different parameter $\\eta$ ( pronounced \"eta\", you'll see other places refer to it as $\\beta$ ).\n",
        "\n",
        "$$ \\phi \\sim Dirichlet(\\eta) $$\n",
        "\n",
        "The utmost goal of LDA is to estimate the $\\theta$ and $\\phi$ which is equivalent to estimate which words are important for which topic and which topics are important for a particular document, respectively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3qKZmwOHNyo",
        "colab_type": "text"
      },
      "source": [
        "![Texte alternatif…](https://raw.githubusercontent.com/Yachad/IASD_TD_1/master/lda.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq0zkD98HUi1",
        "colab_type": "text"
      },
      "source": [
        "The basic idea behind the parameters for the Dirichlet distribution is: $\\alpha$ The higher the value the more likely each document is to contain a mixture of most of the topics instead of any single topic. The same goes for $\\eta$, where higher value denotes that each topic is likely to contain a mixture of most of the words and not any word specifically.\n",
        "\n",
        "There're different approaches to this algorithm, the one we'll be using is gibbs sampling.\n",
        "\n",
        "Gibbs sampling is commonly used as a means of statistical inference, especially Bayesian inference. It is a randomized algorithm (i.e. an algorithm that makes use of random numbers), and is an alternative to deterministic algorithms for statistical inference such as the expectation-maximization algorithm (EM).\n",
        "\n",
        "We'll use 8 short strings to represent our set of documents. \n",
        "The following section creates the set the documents and convert each document into word ids, where word ids is just the ids assigned to each unique word in the set of document. We're dropping the issue of stemming words as this is a fairly simple set of document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIL5W-02_ghz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rawdocs = ['eat turkey on turkey day holiday',\n",
        "          'i like to eat cake on holiday',\n",
        "          'turkey trot race on thanksgiving holiday',\n",
        "          \"snail race the turtle\",\n",
        "          'time travel space race',\n",
        "          'movie on thanksgiving',\n",
        "          'movie at air and space museum is cool movie',\n",
        "          'aspiring movie star'\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "docs = [rawdoc.split(' ') for rawdoc in rawdocs]\n",
        "\n",
        "\n",
        "# unique words\n",
        "vocab = list(set(sum(docs, [])))\n",
        "vocab.sort()\n",
        "vocab = np.array(vocab)\n",
        "\n",
        "\n",
        "# replace words in documents with wordIDs\n",
        "docs_index = [[np.argwhere(vocab == word).item() for word in doc] for doc in docs] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRkpGbZo_gh1",
        "colab_type": "code",
        "outputId": "64e23a1c-8295-42ca-f71a-feb200678f2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "docs"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['eat', 'turkey', 'on', 'turkey', 'day', 'holiday'],\n",
              " ['i', 'like', 'to', 'eat', 'cake', 'on', 'holiday'],\n",
              " ['turkey', 'trot', 'race', 'on', 'thanksgiving', 'holiday'],\n",
              " ['snail', 'race', 'the', 'turtle'],\n",
              " ['time', 'travel', 'space', 'race'],\n",
              " ['movie', 'on', 'thanksgiving'],\n",
              " ['movie', 'at', 'air', 'and', 'space', 'museum', 'is', 'cool', 'movie'],\n",
              " ['aspiring', 'movie', 'star']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAT2z3MLNg9X",
        "colab_type": "code",
        "outputId": "ba83c968-1f96-4fde-eae2-0d713ac1704e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "docs_index"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[7, 25, 14, 25, 6, 8],\n",
              " [9, 11, 22, 7, 4, 14, 8],\n",
              " [25, 24, 15, 14, 19, 8],\n",
              " [16, 15, 20, 26],\n",
              " [21, 23, 17, 15],\n",
              " [12, 14, 19],\n",
              " [12, 3, 0, 1, 17, 13, 10, 5, 12],\n",
              " [2, 12, 18]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9rGKXFP_gh4",
        "colab_type": "code",
        "outputId": "bc87905e-c47a-4899-cc21-024e2f9bb2c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "vocab"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['air', 'and', 'aspiring', 'at', 'cake', 'cool', 'day', 'eat',\n",
              "       'holiday', 'i', 'is', 'like', 'movie', 'museum', 'on', 'race',\n",
              "       'snail', 'space', 'star', 'thanksgiving', 'the', 'time', 'to',\n",
              "       'travel', 'trot', 'turkey', 'turtle'], dtype='<U12')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R58WnzGQ_gh6",
        "colab_type": "text"
      },
      "source": [
        "A slight drawback of latent dirichlet allocation is that you have to specify the number of clusters first. In other words you have to specify the number of topics that you wish to group the set of documents into upfront ( denoted by K ). In our cases we'll use 2.\n",
        "\n",
        "The first step of the algorithm is to go through each document and randomly assign each word in the document to one of the K topics. Apart from generating this **topic assignment list**, we'll also create a **word-topic matrix**, which is the count of each word being assigned to each topic. And a **document-topic matrix**, which is the number of words assigned to each topic for each document (distribution of the topic assignment list). We'll be using the later two matrices throughout the process of the algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoZ4IuhC_gh6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cluster number \n",
        "K = 2\n",
        "\n",
        "# %%\n",
        "# initialize count matrices \n",
        "# @wt = word-topic matrix \n",
        "wt = pd.DataFrame(data = np.zeros(shape = [K, len(vocab)]),\n",
        "                  columns= vocab,\n",
        "                  index= np.arange(1, K+1))\n",
        "\n",
        "\n",
        "# @ta : topic assignment list\n",
        "ta = [[np.random.randint(1, high=K+1, size=1).item() for word in doc] for doc in docs]\n",
        "\n",
        "# %%\n",
        "# @dt : counts correspond to the number of words assigned to each topic for each document\n",
        "dt = np.zeros([len(docs), K])\n",
        "\n",
        "\n",
        "for index_doc, doc in enumerate(docs):\n",
        "    # randomly assign topic to word w\n",
        "    for index_word,  word in enumerate(doc):\n",
        "        ta[index_doc][index_word] = np.random.randint(1, high=K+1, size=1).item()\n",
        "        \n",
        "        # extract the topic index, word id and update the corresponding cell\n",
        "        # in the word-topic matrix\n",
        "        topic = ta[index_doc][index_word]\n",
        "        wt.loc[topic, word] +=  1\n",
        "\n",
        "    # count words in document d assigned to each topic t\n",
        "    for t in np.arange(1, K+1) :\n",
        "        dt[index_doc,t-1] =  np.sum((np.array(ta[index_doc]) == t) * 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15XX0qED_gh8",
        "colab_type": "code",
        "outputId": "37fa28cf-1583-42fa-e1b1-19f302a4817f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "wt"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>air</th>\n",
              "      <th>and</th>\n",
              "      <th>aspiring</th>\n",
              "      <th>at</th>\n",
              "      <th>cake</th>\n",
              "      <th>cool</th>\n",
              "      <th>day</th>\n",
              "      <th>eat</th>\n",
              "      <th>holiday</th>\n",
              "      <th>i</th>\n",
              "      <th>is</th>\n",
              "      <th>like</th>\n",
              "      <th>movie</th>\n",
              "      <th>museum</th>\n",
              "      <th>on</th>\n",
              "      <th>race</th>\n",
              "      <th>snail</th>\n",
              "      <th>space</th>\n",
              "      <th>star</th>\n",
              "      <th>thanksgiving</th>\n",
              "      <th>the</th>\n",
              "      <th>time</th>\n",
              "      <th>to</th>\n",
              "      <th>travel</th>\n",
              "      <th>trot</th>\n",
              "      <th>turkey</th>\n",
              "      <th>turtle</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   air  and  aspiring   at  cake  cool  ...  time   to  travel  trot  turkey  turtle\n",
              "1  0.0  0.0       1.0  1.0   1.0   1.0  ...   1.0  0.0     0.0   1.0     2.0     0.0\n",
              "2  1.0  1.0       0.0  0.0   0.0   0.0  ...   0.0  1.0     1.0   0.0     1.0     1.0\n",
              "\n",
              "[2 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STiXa4-U_gh-",
        "colab_type": "code",
        "outputId": "25137c01-84f7-4da3-deb3-57a958c0050c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "ta"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 1, 2, 2, 2, 1],\n",
              " [1, 1, 2, 1, 1, 1, 1],\n",
              " [1, 1, 2, 1, 1, 1],\n",
              " [2, 1, 2, 2],\n",
              " [1, 2, 1, 2],\n",
              " [1, 2, 2],\n",
              " [2, 1, 2, 2, 2, 2, 2, 1, 2],\n",
              " [1, 2, 2]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxMBUt3y_gh_",
        "colab_type": "code",
        "outputId": "bc8e901e-0339-41d6-8070-00a011693269",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "dt"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3., 3.],\n",
              "       [6., 1.],\n",
              "       [5., 1.],\n",
              "       [1., 3.],\n",
              "       [2., 2.],\n",
              "       [1., 2.],\n",
              "       [2., 7.],\n",
              "       [1., 2.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47qy9JDb_giF",
        "colab_type": "text"
      },
      "source": [
        "Notice that this random assignment already gives you both the topic representations of all the documents and word distributions of all the topics, albeit not very good ones. \n",
        "\n",
        "So to improve them, we'll employ the gibbs sampling method that performs the following steps for a user-specified iteration: \n",
        "\n",
        "For each document d, go through each word w (a double for loop). Reassign a new topic to w, where we choose topic t with the probability of word w given topic t $\\times$ probability of topic t given document d, denoted by the following mathematical notations:\n",
        "\n",
        "$$ P( z_i = j \\text{ }| \\text{ } z_{-i}, w_i, d_i ) \n",
        "    \\propto  \\frac{ C^{WT}_{w_ij} + \\eta }{ \\sum^W_{ w = 1 }C^{WT}_{wj} + W\\eta } \\times\n",
        "      \\frac{ C^{DT}_{d_ij} + \\alpha }{ \\sum^T_{ t = 1 }C^{DT}_{d_it} + T\\alpha }\n",
        "$$\n",
        "\n",
        "\n",
        "Let's try and break that down piece by piece..... \n",
        "\n",
        "Starting from the left side of the equal sign:\n",
        "\n",
        "- **$P(z_i = j)$ :** The probability that token i is assigned to topic j.\n",
        "- **$z_{-i}$ :** Represents topic assignments of all other tokens.\n",
        "- **$w_i$ :** Word (index) of the $i_{th}$ token.\n",
        "- **$d_i$ :** Document containing the $i_{th}$ token.\n",
        "\n",
        "For the right side of the proportionality :\n",
        "\n",
        "- **$C^{WT}$ :** Word-topic matrix, the `wt` matrix we generated.\n",
        "- **$\\sum^W_{ w = 1 }C^{WT}_{wj}$ :** Total number of tokens (words) in each topic.\n",
        "- **$C^{DT}$ :** Document-topic matrix, the `dt` matrix we generated.\n",
        "- **$\\sum^T_{ t = 1 }C^{DT}_{d_it}$ :** Total number of tokens (words) in document i.\n",
        "- **$\\eta$ :** Parameter that sets the topic distribution for the words, the higher the more spread out the words will be across the specified number of topics (K). \n",
        "- **$\\alpha$ :** Parameter that sets the topic distribution for the documents, the higher the more spread out the documents will be across the specified number of topics (K).\n",
        "- **$W$ :** Total number of words in the set of documents. \n",
        "- **$T$ :** Number of topics, equivalent of the K we defined earlier. \n",
        "\n",
        "It may be still confusing with all of that notations, the following section goes through the computation for one iteration. The topic of the first word in the first document is resampled as follow: The output will not be printed during the process, since it'll probably make the documentation messier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiS4rQpkKugf",
        "colab_type": "text"
      },
      "source": [
        "LDA Gibbs Algorithm \n",
        "\n",
        "\n",
        "#### for each iteration $i $ :\n",
        "#### &nbsp;&nbsp;  for each document  $d_i  \\in \\mathcal D $ :\n",
        "#### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  for each word  $w_i  \\in \\mathcal d_i $ :\n",
        "#### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; $z_{old}$ : topic assigned to $w_i$ \n",
        "#### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.&nbsp;&nbsp;&nbsp; Decrement &nbsp;&nbsp;   $n_{d_i, z_{old}}$, $wt_{z_{old}, w_{d, n}}$ \n",
        "\n",
        "#### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.&nbsp;&nbsp;&nbsp; Sample $z_{new}$ &nbsp;&nbsp; &nbsp;&nbsp; from $ P( z_i = j \\text{ }| \\text{ } z_{-i}, w_i, d_i ) $ \n",
        "\n",
        "#### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.&nbsp;&nbsp;&nbsp; Increment &nbsp;&nbsp;   $n_{d_i, z_{new}}$, $wt_{z_{new}, w_{d, n}}$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf0BXBGOiUNK",
        "colab_type": "text"
      },
      "source": [
        "Gibbs sampling one iteration "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yN4RmNHi_giG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 1. Randomly assign topics to\n",
        "alpha = 1\n",
        "eta = 1\n",
        "\n",
        "\n",
        "# initial topics assigned to the first word of the first document \n",
        "# and its corresponding word id \n",
        "\n",
        "t0 = ta[0][0]\n",
        "word_id = docs_index[0][0]\n",
        "\n",
        "# Z_-i means that we dot not include token w in our word-topic and document-topic\n",
        "# count matrix when sampling for token w,\n",
        "# only leave the topic assignments of all other tokens for document 1\n",
        "\n",
        "## ! decrement dt for the first document and topic t0 \n",
        "..\n",
        "## ! decrement wt for the  topic t0  and word_id\n",
        "..\n",
        "\n",
        "# calculate left side and right side of propo\n",
        "\n",
        "### ! calculate the left side of the probability\n",
        "left = ..\n",
        "\n",
        "### ! calculate the right side of the probability for the first document\n",
        "right = \n",
        "\n",
        "# transform the proportionality to a probability\n",
        "prob_topic = (left * right) /(left * right).sum()\n",
        "\n",
        "# %%\n",
        "# draw new topic for the first word in the first document \n",
        "### ! draw a new topic with the probability proba_topic\n",
        "new_t0 = ..\n",
        "\n",
        "# refresh the dt and wt with the newly assigned topic \n",
        "###! refresh the value of the topic in  ta for the first first doc and the first word \n",
        "...\n",
        "\n",
        "## ! increment dt for the first document and topic new_t0 \n",
        "..\n",
        "## ! increment wt for the  topic new_t0   and word_id\n",
        ".."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZQkIAbR_giH",
        "colab_type": "text"
      },
      "source": [
        "After the first iteration, the topic for the first word in the first document is updated to `r new_t0`. Hopefully, that is clears out the confusing of all those mathematical notations. We can now apply the whole thing to a user-specified iteration. Just remember after drawing the new topic we also have to update the topic assignment list with newly sampled topic for token w; re-increment the word-topic and document-topic count matrices with the new sampled topic for token w.\n",
        "\n",
        "To conserve space, we'll put all of it into a function [`LDA1`][LDA], which takes the paramters of:\n",
        "\n",
        "- `docs` Document that have be converted to token (word) ids.\n",
        "- `vocab` Unique tokens (words) for all the document collection.\n",
        "- `K` Number of topic groups.\n",
        "- `alpha` and `eta` Distribution parameters as explained earlier.\n",
        "- `iterations` Number of iterations to run gibbs sampling to train our model.\n",
        "- Returns a list containing the final weight-topic count matrix `wt` and document-topic matrix `dt`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq4_yGay_giI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lda_scratch(docs, vocab, K, alpha, eta, iterations):\n",
        "\n",
        "    # replace words in documents with wordIDs\n",
        "    docs_index = [[np.argwhere(vocab == word).item() for word in doc] for doc in docs] \n",
        "\n",
        "    #initialize count matrices\n",
        "    # @wt : word-topic matrix\n",
        "    wt = \n",
        "\n",
        "    # @ta : topic assignment list\n",
        "    ta = \n",
        "\n",
        "    # @dt : counts correspond to the number of words assigned to each topic for each document\n",
        "    dt = \n",
        "\n",
        "    for index_doc, doc in enumerate(docs):\n",
        "    # randomly assign topic to word w\n",
        "        for index_word,  word in enumerate(doc):\n",
        "            \n",
        "            # extract the topic index, word id and update the corresponding cell\n",
        "            # in the word-topic matrix\n",
        "            topic = \n",
        "            wt.loc[topic, word] =  \n",
        "\n",
        "            # count words in document d assigned to each topic t\n",
        "            for t in np.arange(1, K+1) :\n",
        "                dt[index_doc,t-1] =  \n",
        "\n",
        "\n",
        "    # for each pass through the corpus\n",
        "\n",
        "    for i in tqdm(np.arange(iterations)):\n",
        "\n",
        "        # for each document \n",
        "        for id_doc in np.arange(len(docs)):\n",
        "            \n",
        "            # for each id_word in id_doc\n",
        "            for id_word in np.arange(len(docs[id_doc])):\n",
        "\n",
        "                # initial topics assigned to the first word of the first document \n",
        "                # and its corresponding word id \n",
        "\n",
        "                ti = \n",
        "                word_id = \n",
        "\n",
        "                dt[id_doc, ti-1] = \n",
        "                wt.iloc[ti-1, word_id] = \n",
        "\n",
        "                left = \n",
        "                right =\n",
        "\n",
        "\n",
        "                # transform the proportionality to a probability\n",
        "                prob_topic = \n",
        "\n",
        "                # draw new topic for id_word in the id_doc document \n",
        "                new_ti = \n",
        "\n",
        "                # refresh the dt and wt with the newly assigned topic \n",
        "                ta[id_doc][id_word] = \n",
        "                dt[id_doc, new_ti-1] = \n",
        "                wt.iloc[new_ti-1, word_id] = \n",
        "    \n",
        "    return wt, dt "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnQTJxM2_giJ",
        "colab_type": "code",
        "outputId": "9b35bc17-6ab8-4c4e-ed7b-48b36eebd58c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "K=2\n",
        "alpha = 1\n",
        "eta = 0.001\n",
        "iterations = 1000\n",
        "\n",
        "# %%\n",
        "wt , dt = lda_scratch(docs, vocab, K, alpha, eta, iterations)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [01:31<00:00, 10.88it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMwzDURy_giL",
        "colab_type": "text"
      },
      "source": [
        "After we're done with learning the topics for `r iterations` iterations, we can use the count matrices to obtain the word-topic distribution and document-topic distribution.\n",
        "\n",
        "To compute the probability of word given topic:\n",
        "\n",
        "$$\\phi_{ij} = \\frac{C^{WT}_{ij} + \\eta}{\\sum^W_{ k = 1 }C^{WT}_{kj} + W\\eta}$$\n",
        "\n",
        "Where $\\phi_{ij}$ is the probability of word i for topic j."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JldZgthD_giM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# topic probability of every word\n",
        "\n",
        "#! calculate the phi value\n",
        "phi_values = \n",
        "phi = pd.DataFrame(data= phi_values,\n",
        "                   columns= vocab,\n",
        "                   index= np.arange(1, K+1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEnFDQKA_giO",
        "colab_type": "code",
        "outputId": "32019a20-36a8-4a22-8d1a-1c44ad4e1a7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "phi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>air</th>\n",
              "      <th>and</th>\n",
              "      <th>aspiring</th>\n",
              "      <th>at</th>\n",
              "      <th>cake</th>\n",
              "      <th>cool</th>\n",
              "      <th>day</th>\n",
              "      <th>eat</th>\n",
              "      <th>holiday</th>\n",
              "      <th>i</th>\n",
              "      <th>is</th>\n",
              "      <th>like</th>\n",
              "      <th>movie</th>\n",
              "      <th>museum</th>\n",
              "      <th>on</th>\n",
              "      <th>race</th>\n",
              "      <th>snail</th>\n",
              "      <th>space</th>\n",
              "      <th>star</th>\n",
              "      <th>thanksgiving</th>\n",
              "      <th>the</th>\n",
              "      <th>time</th>\n",
              "      <th>to</th>\n",
              "      <th>travel</th>\n",
              "      <th>trot</th>\n",
              "      <th>turkey</th>\n",
              "      <th>turtle</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.047605</td>\n",
              "      <td>0.047605</td>\n",
              "      <td>0.047605</td>\n",
              "      <td>0.095163</td>\n",
              "      <td>0.142721</td>\n",
              "      <td>0.047605</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.047605</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.190279</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.095163</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.047605</td>\n",
              "      <td>0.047605</td>\n",
              "      <td>0.142721</td>\n",
              "      <td>0.000048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.047605</td>\n",
              "      <td>0.047605</td>\n",
              "      <td>0.047605</td>\n",
              "      <td>0.047605</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.047605</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.190279</td>\n",
              "      <td>0.047605</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.142721</td>\n",
              "      <td>0.047605</td>\n",
              "      <td>0.095163</td>\n",
              "      <td>0.047605</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.047605</td>\n",
              "      <td>0.047605</td>\n",
              "      <td>0.047605</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.047605</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        air       and  aspiring  ...      trot    turkey    turtle\n",
              "1  0.000048  0.000048  0.000048  ...  0.047605  0.142721  0.000048\n",
              "2  0.047605  0.047605  0.047605  ...  0.000048  0.000048  0.047605\n",
              "\n",
              "[2 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhQUNNMh_giP",
        "colab_type": "text"
      },
      "source": [
        "$$\\theta_{dj} = \\frac{C^{DT}_{dj} + \\alpha}{\\sum^T_{ k = 1 }C^{DT}_{dk} + T\\alpha}$$\n",
        "\n",
        "Where $\\theta_{dj}$ is the proportion of topic j in document d.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zxWMV65_giQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# topic probability of every document\n",
        "## calculate the theta value\n",
        "theta = "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P60G_0SM_giR",
        "colab_type": "code",
        "outputId": "55cebe66-195f-43ce-d854-b549889e6ae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "theta"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.875     , 0.125     ],\n",
              "       [0.77777778, 0.22222222],\n",
              "       [0.75      , 0.25      ],\n",
              "       [0.16666667, 0.83333333],\n",
              "       [0.33333333, 0.66666667],\n",
              "       [0.6       , 0.4       ],\n",
              "       [0.18181818, 0.81818182],\n",
              "       [0.2       , 0.8       ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sGYhRak_giT",
        "colab_type": "text"
      },
      "source": [
        "Recall that LDA assumes that each document is a mixture of all topics, thus after computing the probability that each document belongs to each topic ( same goes for word & topic ) we can use this information to see which topic does each document belongs to and the more possible words that are associated with each topic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ_AbTZX_giU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# topic assigned to each document, the one with the highest probability\n",
        "topics = np.array([np.argmax(doc_prob) + 1 for doc_prob in theta])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGJiehRd_giV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# possible words under each topic\n",
        "# sort the probability and obtain the user-specified number n\n",
        "def find_top_term (phi, n):\n",
        "    top_term = {}\n",
        "    for topic in phi.index.values :\n",
        "        top_term[str(topic)] = phi.loc[topic].sort_values(ascending=False)[:n]\n",
        "\n",
        "    return top_term\n",
        "\n",
        "top_terms = find_top_term(phi, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9dL5C1r_giY",
        "colab_type": "text"
      },
      "source": [
        "We specified that we wanted to see the top 3 terms associated with each topic. The following section prints out the original raw document, which is grouped into `r K` groups that we specified and words that are likely to go along with each topic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9wXbiLC_giZ",
        "colab_type": "code",
        "outputId": "e92ef0e4-357b-45ad-d585-def57004b540",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "### Topic \n",
        "topic = 1\n",
        "for i, rawdoc in enumerate(rawdocs):\n",
        "    if topics[i] == topic:\n",
        "        print(rawdoc)\n",
        "        \n",
        "top_terms[str(topic)]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eat turkey on turkey day holiday\n",
            "i like to eat cake on holiday\n",
            "turkey trot race on thanksgiving holiday\n",
            "movie on thanksgiving\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "on         0.190279\n",
              "turkey     0.142721\n",
              "holiday    0.142721\n",
              "Name: 1, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKH36Asz_gib",
        "colab_type": "code",
        "outputId": "755b077d-6ef9-45c3-cc52-20b64b8ed0d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "### Topic \n",
        "topic = 2\n",
        "for i, rawdoc in enumerate(rawdocs):\n",
        "    if topics[i] == topic:\n",
        "        print(rawdoc)\n",
        "        \n",
        "top_terms[str(topic)]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "snail race the turtle\n",
            "time travel space race\n",
            "movie at air and space museum is cool movie\n",
            "aspiring movie star\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "movie    0.190279\n",
              "race     0.142721\n",
              "space    0.095163\n",
              "Name: 2, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4nvbx00_gid",
        "colab_type": "text"
      },
      "source": [
        "The output tells us that the first topic seems to be discussing something about movie and race , while the second is something about turkey and holiday. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54sO0qov_gid",
        "colab_type": "text"
      },
      "source": [
        "### Yelp review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycRjDhsF_gie",
        "colab_type": "text"
      },
      "source": [
        "In this tutorial, we will take a real example of the ’YelP reviews’ dataset and use LDA to extract the naturally discussed topics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByhTv49G_gie",
        "colab_type": "code",
        "outputId": "1079c18d-3264-45ac-e2bc-794958b3adf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk; nltk.download('stopwords')\n",
        "#!pip3 install  spacy download en"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr10fhU6_gig",
        "colab_type": "code",
        "outputId": "eadf25a5-8b36-48dd-bc4f-b9a90d0bff6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# spacy for lemmatization\n",
        "import spacy\n",
        "\n",
        "# Plotting tools\n",
        "!pip install pyldavis\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim  # don't skip this\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Enable logging for gensim - optional\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyldavis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n",
            "\r\u001b[K     |▏                               | 10kB 14.4MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 3.3MB/s eta 0:00:01\r\u001b[K     |▋                               | 30kB 4.5MB/s eta 0:00:01\r\u001b[K     |▉                               | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |█                               | 51kB 3.7MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61kB 4.4MB/s eta 0:00:01\r\u001b[K     |█▍                              | 71kB 5.0MB/s eta 0:00:01\r\u001b[K     |█▋                              | 81kB 5.6MB/s eta 0:00:01\r\u001b[K     |█▉                              | 92kB 6.3MB/s eta 0:00:01\r\u001b[K     |██                              | 102kB 5.0MB/s eta 0:00:01\r\u001b[K     |██▎                             | 112kB 5.0MB/s eta 0:00:01\r\u001b[K     |██▍                             | 122kB 5.0MB/s eta 0:00:01\r\u001b[K     |██▋                             | 133kB 5.0MB/s eta 0:00:01\r\u001b[K     |██▉                             | 143kB 5.0MB/s eta 0:00:01\r\u001b[K     |███                             | 153kB 5.0MB/s eta 0:00:01\r\u001b[K     |███▎                            | 163kB 5.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 174kB 5.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 184kB 5.0MB/s eta 0:00:01\r\u001b[K     |███▉                            | 194kB 5.0MB/s eta 0:00:01\r\u001b[K     |████                            | 204kB 5.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 215kB 5.0MB/s eta 0:00:01\r\u001b[K     |████▌                           | 225kB 5.0MB/s eta 0:00:01\r\u001b[K     |████▊                           | 235kB 5.0MB/s eta 0:00:01\r\u001b[K     |████▉                           | 245kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 256kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 266kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 276kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 286kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 296kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 307kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 317kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 327kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 337kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 348kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 358kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 368kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 378kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 389kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 399kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 409kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 419kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 430kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 440kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 450kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 460kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 471kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 481kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 491kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 501kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 512kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 522kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 532kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 542kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 552kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 563kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 573kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 583kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 593kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 604kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 614kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 624kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 634kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 645kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 655kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 665kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 675kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 686kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 696kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 706kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 716kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 727kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 737kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 747kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 757kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 768kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 778kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 788kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 798kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 808kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 819kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 829kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 839kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 849kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 860kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 870kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 880kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 890kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 901kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 911kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 921kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 931kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 942kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 952kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 962kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 972kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 983kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 993kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.0MB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.0MB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.0MB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.0MB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.0MB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.1MB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.1MB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.1MB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.1MB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.1MB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1MB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.1MB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.1MB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.1MB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.1MB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.2MB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.2MB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.2MB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.2MB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.2MB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.2MB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.2MB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.2MB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.2MB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.2MB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.3MB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.3MB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.3MB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.3MB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.3MB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.3MB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.3MB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.3MB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.3MB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.4MB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.4MB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.4MB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.4MB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.4MB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.4MB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.4MB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.4MB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.4MB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.4MB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.5MB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.5MB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.5MB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.5MB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.5MB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.5MB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.5MB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.5MB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.5MB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.5MB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.6MB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.6MB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.6MB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.6MB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.6MB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.6MB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6MB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyldavis) (0.33.6)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyldavis) (1.17.5)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyldavis) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pyldavis) (0.25.3)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyldavis) (0.14.1)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyldavis) (2.10.3)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyldavis) (2.7.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyldavis) (3.6.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyldavis) (0.16.0)\n",
            "Collecting funcy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/4b/6ffa76544e46614123de31574ad95758c421aae391a1764921b8a81e1eae/funcy-1.14.tar.gz (548kB)\n",
            "\r\u001b[K     |▋                               | 10kB 18.6MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 23.3MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30kB 28.9MB/s eta 0:00:01\r\u001b[K     |██▍                             | 40kB 33.1MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 36.2MB/s eta 0:00:01\r\u001b[K     |███▋                            | 61kB 39.5MB/s eta 0:00:01\r\u001b[K     |████▏                           | 71kB 41.5MB/s eta 0:00:01\r\u001b[K     |████▉                           | 81kB 42.2MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 92kB 43.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 102kB 34kB/s eta 0:00:14\r\u001b[K     |██████▋                         | 112kB 34kB/s eta 0:00:13\r\u001b[K     |███████▏                        | 122kB 34kB/s eta 0:00:13\r\u001b[K     |███████▊                        | 133kB 34kB/s eta 0:00:13\r\u001b[K     |████████▍                       | 143kB 34kB/s eta 0:00:12\r\u001b[K     |█████████                       | 153kB 34kB/s eta 0:00:12\r\u001b[K     |█████████▋                      | 163kB 34kB/s eta 0:00:12\r\u001b[K     |██████████▏                     | 174kB 34kB/s eta 0:00:11\r\u001b[K     |██████████▊                     | 184kB 34kB/s eta 0:00:11\r\u001b[K     |███████████▍                    | 194kB 34kB/s eta 0:00:11\r\u001b[K     |████████████                    | 204kB 34kB/s eta 0:00:11\r\u001b[K     |████████████▌                   | 215kB 34kB/s eta 0:00:10\r\u001b[K     |█████████████▏                  | 225kB 34kB/s eta 0:00:10\r\u001b[K     |█████████████▊                  | 235kB 34kB/s eta 0:00:10\r\u001b[K     |██████████████▍                 | 245kB 34kB/s eta 0:00:09\r\u001b[K     |███████████████                 | 256kB 34kB/s eta 0:00:09\r\u001b[K     |███████████████▌                | 266kB 34kB/s eta 0:00:09\r\u001b[K     |████████████████▏               | 276kB 34kB/s eta 0:00:08\r\u001b[K     |████████████████▊               | 286kB 34kB/s eta 0:00:08\r\u001b[K     |█████████████████▎              | 296kB 34kB/s eta 0:00:08\r\u001b[K     |██████████████████              | 307kB 34kB/s eta 0:00:08\r\u001b[K     |██████████████████▌             | 317kB 34kB/s eta 0:00:07\r\u001b[K     |███████████████████▏            | 327kB 34kB/s eta 0:00:07\r\u001b[K     |███████████████████▊            | 337kB 34kB/s eta 0:00:07\r\u001b[K     |████████████████████▎           | 348kB 34kB/s eta 0:00:06\r\u001b[K     |█████████████████████           | 358kB 34kB/s eta 0:00:06\r\u001b[K     |█████████████████████▌          | 368kB 34kB/s eta 0:00:06\r\u001b[K     |██████████████████████          | 378kB 34kB/s eta 0:00:05\r\u001b[K     |██████████████████████▊         | 389kB 34kB/s eta 0:00:05\r\u001b[K     |███████████████████████▎        | 399kB 34kB/s eta 0:00:05\r\u001b[K     |████████████████████████        | 409kB 34kB/s eta 0:00:05\r\u001b[K     |████████████████████████▌       | 419kB 34kB/s eta 0:00:04\r\u001b[K     |█████████████████████████       | 430kB 34kB/s eta 0:00:04\r\u001b[K     |█████████████████████████▊      | 440kB 34kB/s eta 0:00:04\r\u001b[K     |██████████████████████████▎     | 450kB 34kB/s eta 0:00:03\r\u001b[K     |██████████████████████████▉     | 460kB 34kB/s eta 0:00:03\r\u001b[K     |███████████████████████████▌    | 471kB 34kB/s eta 0:00:03\r\u001b[K     |████████████████████████████    | 481kB 34kB/s eta 0:00:02\r\u001b[K     |████████████████████████████▊   | 491kB 34kB/s eta 0:00:02\r\u001b[K     |█████████████████████████████▎  | 501kB 34kB/s eta 0:00:02\r\u001b[K     |█████████████████████████████▉  | 512kB 34kB/s eta 0:00:02\r\u001b[K     |██████████████████████████████▌ | 522kB 34kB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 532kB 34kB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 542kB 34kB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 552kB 34kB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyldavis) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyldavis) (2.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyldavis) (1.1.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyldavis) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyldavis) (8.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyldavis) (42.0.2)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyldavis) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyldavis) (1.12.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyldavis) (1.8.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyldavis) (19.3.0)\n",
            "Building wheels for collected packages: pyldavis, funcy\n",
            "  Building wheel for pyldavis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyldavis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97711 sha256=d9f8ef936bac78f513ffed53c44245050829f3ed31a2a8c86f51e5f9c5f024f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n",
            "  Building wheel for funcy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for funcy: filename=funcy-1.14-py2.py3-none-any.whl size=32040 sha256=2e316a3a84e890cdb0973a98c0b3f66d2fef30204925e863377f1fe6b6da3226\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/5a/d8/1d875df03deae6f178dfdf70238cca33f948ef8a6f5209f2eb\n",
            "Successfully built pyldavis funcy\n",
            "Installing collected packages: funcy, pyldavis\n",
            "Successfully installed funcy-1.14 pyldavis-2.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PanY3AdbGINV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link = 'https://raw.githubusercontent.com/Yachad/IASD_TD_1/master/yelp.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFWVSWs2_gii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(link)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ViWT5PL_gij",
        "colab_type": "code",
        "outputId": "b8f6b12f-281f-41bf-e9c5-d713a891931b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Convert to list\n",
        "data = df.text.values.tolist()\n",
        "\n",
        "# Remove Emails\n",
        "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
        "\n",
        "# Remove new line characters\n",
        "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
        "\n",
        "# Remove distracting single quotes\n",
        "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
        "\n",
        "pprint(data[:1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['My wife took me here on my birthday for breakfast and it was excellent. The '\n",
            " 'weather was perfect which made sitting outside overlooking their grounds an '\n",
            " 'absolute pleasure. Our waitress was excellent and our food arrived quickly '\n",
            " 'on the semi-busy Saturday morning. It looked like the place fills up pretty '\n",
            " 'quickly so the earlier you get here the better. Do yourself a favor and get '\n",
            " 'their Bloody Mary. It was phenomenal and simply the best Ive ever had. Im '\n",
            " 'pretty sure they only use ingredients from their garden and blend them fresh '\n",
            " 'when you order it. It was amazing. While EVERYTHING on the menu looks '\n",
            " 'excellent, I had the white truffle scrambled eggs vegetable skillet and it '\n",
            " 'was tasty and delicious. It came with 2 pieces of their griddled bread with '\n",
            " 'was amazing and it absolutely made the meal complete. It was the best '\n",
            " '\"toast\" Ive ever had. Anyway, I cant wait to go back!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE5uUEb0_gik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NLTK Stop words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYEirOSv_gim",
        "colab_type": "code",
        "outputId": "98c255e5-6bd7-49c3-c14b-92ef8272cff6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "print(data_words[:1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['my', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'was', 'excellent', 'the', 'weather', 'was', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', 'grounds', 'an', 'absolute', 'pleasure', 'our', 'waitress', 'was', 'excellent', 'and', 'our', 'food', 'arrived', 'quickly', 'on', 'the', 'semi', 'busy', 'saturday', 'morning', 'it', 'looked', 'like', 'the', 'place', 'fills', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'do', 'yourself', 'favor', 'and', 'get', 'their', 'bloody', 'mary', 'it', 'was', 'phenomenal', 'and', 'simply', 'the', 'best', 'ive', 'ever', 'had', 'im', 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'it', 'was', 'amazing', 'while', 'everything', 'on', 'the', 'menu', 'looks', 'excellent', 'had', 'the', 'white', 'truffle', 'scrambled', 'eggs', 'vegetable', 'skillet', 'and', 'it', 'was', 'tasty', 'and', 'delicious', 'it', 'came', 'with', 'pieces', 'of', 'their', 'griddled', 'bread', 'with', 'was', 'amazing', 'and', 'it', 'absolutely', 'made', 'the', 'meal', 'complete', 'it', 'was', 'the', 'best', 'toast', 'ive', 'ever', 'had', 'anyway', 'cant', 'wait', 'to', 'go', 'back']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otkb8TQU_gin",
        "colab_type": "code",
        "outputId": "6ffb4d0c-fade-4c76-eabe-e47cbb1e6050",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "# See trigram example\n",
        "print(trigram_mod[bigram_mod[data_words[0]]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['my', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'was', 'excellent', 'the', 'weather', 'was', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', 'grounds', 'an', 'absolute', 'pleasure', 'our', 'waitress', 'was', 'excellent', 'and', 'our', 'food', 'arrived', 'quickly', 'on', 'the', 'semi', 'busy', 'saturday', 'morning', 'it', 'looked', 'like', 'the', 'place', 'fills', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'do_yourself_favor', 'and', 'get', 'their', 'bloody_mary', 'it', 'was', 'phenomenal', 'and', 'simply', 'the', 'best', 'ive', 'ever', 'had', 'im', 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'it', 'was', 'amazing', 'while', 'everything', 'on', 'the', 'menu', 'looks', 'excellent', 'had', 'the', 'white', 'truffle', 'scrambled_eggs', 'vegetable', 'skillet', 'and', 'it', 'was', 'tasty', 'and', 'delicious', 'it', 'came', 'with', 'pieces', 'of', 'their', 'griddled', 'bread', 'with', 'was', 'amazing', 'and', 'it', 'absolutely', 'made', 'the', 'meal', 'complete', 'it', 'was', 'the', 'best', 'toast', 'ive', 'ever', 'had', 'anyway', 'cant', 'wait', 'to', 'go', 'back']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AdE-TwF_gip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU6WmD8I_gir",
        "colab_type": "code",
        "outputId": "99076999-c037-416b-c2e7-c7127a3f3cba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Remove Stop Words\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "# python3 -m spacy download en\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['wife', 'take', 'birthday', 'breakfast', 'excellent', 'weather', 'perfect', 'make', 'sit', 'overlooking', 'ground', 'absolute', 'pleasure', 'waitress', 'excellent', 'food', 'arrive', 'quickly', 'semi', 'busy', 'saturday', 'morning', 'look', 'place', 'fill', 'pretty', 'quickly', 'earlier', 'get', 'well', 'favor', 'get', 'bloody_mary', 'phenomenal', 'simply', 'good', 'have', 'ever', 'be', 'pretty', 'sure', 'ingredient', 'garden', 'blend', 'fresh', 'order', 'amazing', 'everything', 'menu', 'look', 'excellent', 'white', 'truffle', 'scrambled_eggs', 'vegetable', 'skillet', 'tasty', 'delicious', 'come', 'piece', 'griddle', 'bread', 'amazing', 'absolutely', 'make', 'meal', 'complete', 'good', 'toast', 'have', 'ever', 'anyway', 'not', 'wait', 'go', 'back']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4Vy_k8C_git",
        "colab_type": "code",
        "outputId": "d9021718-992b-4517-aae9-ed605378506c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "print(corpus[:1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(0, 1), (1, 1), (2, 2), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 2), (18, 1), (19, 3), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 2), (26, 1), (27, 2), (28, 1), (29, 1), (30, 2), (31, 1), (32, 2), (33, 2), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 2), (46, 2), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlx-5QwC_giu",
        "colab_type": "code",
        "outputId": "eb68f119-1c5f-44d7-a380-53aff37a4c6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Human readable format of corpus (term-frequency)\n",
        "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('absolute', 1),\n",
              "  ('absolutely', 1),\n",
              "  ('amazing', 2),\n",
              "  ('anyway', 1),\n",
              "  ('arrive', 1),\n",
              "  ('back', 1),\n",
              "  ('be', 1),\n",
              "  ('birthday', 1),\n",
              "  ('blend', 1),\n",
              "  ('bloody_mary', 1),\n",
              "  ('bread', 1),\n",
              "  ('breakfast', 1),\n",
              "  ('busy', 1),\n",
              "  ('come', 1),\n",
              "  ('complete', 1),\n",
              "  ('delicious', 1),\n",
              "  ('earlier', 1),\n",
              "  ('ever', 2),\n",
              "  ('everything', 1),\n",
              "  ('excellent', 3),\n",
              "  ('favor', 1),\n",
              "  ('fill', 1),\n",
              "  ('food', 1),\n",
              "  ('fresh', 1),\n",
              "  ('garden', 1),\n",
              "  ('get', 2),\n",
              "  ('go', 1),\n",
              "  ('good', 2),\n",
              "  ('griddle', 1),\n",
              "  ('ground', 1),\n",
              "  ('have', 2),\n",
              "  ('ingredient', 1),\n",
              "  ('look', 2),\n",
              "  ('make', 2),\n",
              "  ('meal', 1),\n",
              "  ('menu', 1),\n",
              "  ('morning', 1),\n",
              "  ('not', 1),\n",
              "  ('order', 1),\n",
              "  ('overlooking', 1),\n",
              "  ('perfect', 1),\n",
              "  ('phenomenal', 1),\n",
              "  ('piece', 1),\n",
              "  ('place', 1),\n",
              "  ('pleasure', 1),\n",
              "  ('pretty', 2),\n",
              "  ('quickly', 2),\n",
              "  ('saturday', 1),\n",
              "  ('scrambled_eggs', 1),\n",
              "  ('semi', 1),\n",
              "  ('simply', 1),\n",
              "  ('sit', 1),\n",
              "  ('skillet', 1),\n",
              "  ('sure', 1),\n",
              "  ('take', 1),\n",
              "  ('tasty', 1),\n",
              "  ('toast', 1),\n",
              "  ('truffle', 1),\n",
              "  ('vegetable', 1),\n",
              "  ('wait', 1),\n",
              "  ('waitress', 1),\n",
              "  ('weather', 1),\n",
              "  ('well', 1),\n",
              "  ('white', 1),\n",
              "  ('wife', 1)]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlRm2U_O_giw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=8, \n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRUV8EwT_gix",
        "colab_type": "code",
        "outputId": "247ef788-5016-4645-f120-9b8301b78186",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "# Print the Keyword in the 8 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0,\n",
            "  '0.032*\"dog\" + 0.029*\"room\" + 0.024*\"stay\" + 0.016*\"hotel\" + 0.015*\"pool\" + '\n",
            "  '0.014*\"cake\" + 0.014*\"park\" + 0.013*\"parking\" + 0.012*\"mall\" + '\n",
            "  '0.012*\"cookie\"'),\n",
            " (1,\n",
            "  '0.029*\"not\" + 0.022*\"go\" + 0.021*\"get\" + 0.016*\"be\" + 0.016*\"do\" + '\n",
            "  '0.016*\"good\" + 0.015*\"time\" + 0.011*\"come\" + 0.011*\"make\" + 0.011*\"food\"'),\n",
            " (2,\n",
            "  '0.031*\"location\" + 0.016*\"new\" + 0.013*\"local\" + 0.012*\"spend\" + '\n",
            "  '0.010*\"trip\" + 0.009*\"year\" + 0.009*\"smell\" + 0.008*\"pho\" + 0.008*\"book\" + '\n",
            "  '0.008*\"include\"'),\n",
            " (3,\n",
            "  '0.043*\"great\" + 0.039*\"place\" + 0.028*\"food\" + 0.027*\"good\" + 0.025*\"love\" '\n",
            "  '+ 0.015*\"always\" + 0.014*\"bar\" + 0.014*\"drink\" + 0.013*\"friendly\" + '\n",
            "  '0.013*\"night\"'),\n",
            " (4,\n",
            "  '0.032*\"course\" + 0.016*\"able\" + 0.010*\"office\" + 0.010*\"step\" + '\n",
            "  '0.010*\"play\" + 0.007*\"card\" + 0.007*\"foot\" + 0.007*\"color\" + 0.007*\"road\" + '\n",
            "  '0.007*\"cash\"'),\n",
            " (5,\n",
            "  '0.039*\"store\" + 0.012*\"car\" + 0.012*\"class\" + 0.009*\"young\" + '\n",
            "  '0.007*\"cocktail\" + 0.007*\"wear\" + 0.006*\"dress\" + 0.006*\"movie\" + '\n",
            "  '0.006*\"flight\" + 0.006*\"spa\"'),\n",
            " (6,\n",
            "  '0.022*\"chicken\" + 0.021*\"order\" + 0.018*\"sauce\" + 0.017*\"taste\" + '\n",
            "  '0.016*\"salad\" + 0.016*\"dish\" + 0.015*\"sandwich\" + 0.015*\"cheese\" + '\n",
            "  '0.014*\"fry\" + 0.014*\"fresh\"'),\n",
            " (7,\n",
            "  '0.021*\"use\" + 0.019*\"coffee\" + 0.017*\"shop\" + 0.014*\"buy\" + 0.012*\"thank\" + '\n",
            "  '0.012*\"valley\" + 0.010*\"treat\" + 0.009*\"call\" + 0.008*\"world\" + '\n",
            "  '0.008*\"employee\"')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8fKp9QnfWjo",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate topic model \n",
        "\n",
        "Two metrics are used to evaluate the performance of a topic modeling of a set of documents :\n",
        "\n",
        "1. **Perplexity** :\n",
        "    - Perplexity is a statistical measure of how well a probability model predicts a sample. As applied to LDA, for a given value of \n",
        "k, you estimate the LDA model.\n",
        "    - Then given the theoretical word distributions represented by the topics, compare that to the actual topic mixtures, or distribution of words in your documents. \n",
        "    - However, the statistic is somewhat meaningless on its own. The benefit of this statistic comes in comparing perplexity across different models with varying  k. The model with the lowest perplexity is generally considered the “best”.\n",
        "    - ! Optimizing for perplexity may not yield human interpretable topics ! \n",
        "\n",
        "\n",
        "\n",
        "2. **Coherence** :\n",
        "    - Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference.\n",
        "    - C_v measure is a variant to calculate a topic coherence, it's based on a sliding window, one-set segmentation of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAQ4kXi3_giz",
        "colab_type": "code",
        "outputId": "8cba88e1-992e-4785-a7e3-1269e163a912",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Compute Perplexity\n",
        "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Perplexity:  -7.789951700286562\n",
            "\n",
            "Coherence Score:  0.3424798013925847\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah7C44nV_gi0",
        "colab_type": "code",
        "outputId": "9d0af982-8508-42d7-dcfe-4ba7478f1d00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
        "vis"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-9e1628e71544>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pyLDAvis' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_5OsvjDZu88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}